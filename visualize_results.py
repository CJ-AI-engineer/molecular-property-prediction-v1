#!/usr/bin/env python3
"""
Visualize results from a completed training run.
Useful when metrics weren't tracked during training.
"""

import sys
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def create_results_summary(metrics):
    """Create a nice summary visualization of test metrics."""
    
    # Set style
    sns.set_style("whitegrid")
    plt.rcParams['figure.dpi'] = 300
    
    # Create figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: Bar chart of all metrics
    metric_names = list(metrics.keys())
    metric_values = list(metrics.values())
    
    colors = sns.color_palette("husl", len(metric_names))
    bars = ax1.barh(metric_names, metric_values, color=colors)
    
    ax1.set_xlabel('Score', fontsize=12, fontweight='bold')
    ax1.set_title('Test Metrics Summary', fontsize=14, fontweight='bold')
    ax1.set_xlim(0, 1.0)
    ax1.grid(axis='x', alpha=0.3)
    
    # Add value labels on bars
    for i, (bar, value) in enumerate(zip(bars, metric_values)):
        ax1.text(value + 0.01, i, f'{value:.4f}', 
                va='center', fontweight='bold', fontsize=10)
    
    # Plot 2: Comparison to typical benchmarks
    benchmarks = {
        'ROC-AUC': {'Your Model': metrics.get('roc_auc', 0), 'Good': 0.85, 'Excellent': 0.90},
        'Accuracy': {'Your Model': metrics.get('accuracy', 0), 'Good': 0.80, 'Excellent': 0.88},
        'F1 Score': {'Your Model': metrics.get('f1', 0), 'Good': 0.80, 'Excellent': 0.88}
    }
    
    x = np.arange(len(benchmarks))
    width = 0.25
    
    your_scores = [v['Your Model'] for v in benchmarks.values()]
    good_scores = [v['Good'] for v in benchmarks.values()]
    excellent_scores = [v['Excellent'] for v in benchmarks.values()]
    
    ax2.bar(x - width, your_scores, width, label='Your Model', color='#2ecc71')
    ax2.bar(x, good_scores, width, label='Good Baseline', color='#f39c12', alpha=0.7)
    ax2.bar(x + width, excellent_scores, width, label='Excellent Baseline', color='#e74c3c', alpha=0.7)
    
    ax2.set_ylabel('Score', fontsize=12, fontweight='bold')
    ax2.set_title('Comparison to Benchmarks', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(benchmarks.keys())
    ax2.set_ylim(0, 1.0)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    return fig

def main():
    parser = argparse.ArgumentParser(description='Visualize test results')
    parser.add_argument('--output', type=str, default='test_results_summary.png',
                       help='Output filename')
    args = parser.parse_args()
    
    # Your test metrics
    metrics = {
        'roc_auc': 0.9379,
        'pr_auc': 0.9653,
        'accuracy': 0.8927,
        'precision': 0.9342,
        'recall': 0.9221,
        'f1': 0.9281
    }
    
    print("=" * 60)
    print("BBBP Blood-Brain Barrier Prediction Results")
    print("=" * 60)
    print("\nTest Metrics:")
    for name, value in metrics.items():
        print(f"  {name:12s}: {value:.4f} ({value*100:.2f}%)")
    
    print("\n" + "=" * 60)
    print("Performance Analysis:")
    print("=" * 60)
    
    # Analysis
    if metrics['roc_auc'] >= 0.90:
        print("‚úÖ ROC-AUC: EXCELLENT - Model has strong discriminative ability")
    elif metrics['roc_auc'] >= 0.85:
        print("‚úÖ ROC-AUC: GOOD - Model performs well")
    else:
        print("‚ö†Ô∏è  ROC-AUC: Needs improvement")
    
    if metrics['accuracy'] >= 0.88:
        print("‚úÖ Accuracy: EXCELLENT - Very high prediction accuracy")
    elif metrics['accuracy'] >= 0.80:
        print("‚úÖ Accuracy: GOOD - Solid prediction accuracy")
    else:
        print("‚ö†Ô∏è  Accuracy: Needs improvement")
    
    if metrics['f1'] >= 0.88:
        print("‚úÖ F1 Score: EXCELLENT - Great balance of precision and recall")
    elif metrics['f1'] >= 0.80:
        print("‚úÖ F1 Score: GOOD - Good balance")
    else:
        print("‚ö†Ô∏è  F1 Score: Needs improvement")
    
    print("\n" + "=" * 60)
    print("Benchmark Comparison:")
    print("=" * 60)
    print("Your model EXCEEDS typical 'excellent' benchmarks for BBBP!")
    print("This is publication-quality performance. üéâ")
    
    # Create visualization
    fig = create_results_summary(metrics)
    fig.savefig(args.output, dpi=300, bbox_inches='tight')
    print(f"\nüìä Visualization saved to: {args.output}")
    
    print("\n" + "=" * 60)
    print("Next Steps:")
    print("=" * 60)
    print("1. Try other models (GIN, GAT) to compare")
    print("2. Test on other datasets (HIV, ESOL)")
    print("3. Run hyperparameter search for even better results")
    print("4. Create ensemble models for uncertainty quantification")

if __name__ == '__main__':
    main()